#!/usr/bin/env python3
"""
AVESA End-to-End Pipeline Test

This script tests the complete Phase 2 pipeline with real tenant data:
1. Creates test tenant configuration
2. Triggers the pipeline orchestrator
3. Monitors execution progress
4. Validates results
5. Verifies S3 data loading
"""

import boto3
import json
import time
import uuid
from datetime import datetime
from typing import Dict, Any, List

class EndToEndPipelineTester:
    def __init__(self, region='us-east-2', environment='dev'):
        self.region = region
        self.environment = environment
        self.session = boto3.Session(region_name=region)
        
        # Initialize AWS clients
        self.lambda_client = self.session.client('lambda')
        self.stepfunctions_client = self.session.client('stepfunctions')
        self.dynamodb = self.session.resource('dynamodb')
        self.s3_client = self.session.client('s3')
        
        # Get table references
        self.tenant_services_table = self.dynamodb.Table(f'TenantServices-{environment}')
        self.processing_jobs_table = self.dynamodb.Table(f'ProcessingJobs-{environment}')
        self.chunk_progress_table = self.dynamodb.Table(f'ChunkProgress-{environment}')
        
        # Test configuration - using existing sitetechnology tenant
        self.test_tenant_id = "sitetechnology"
        # Note: The actual job ID will be generated by the orchestrator in format: job-YYYYMMDD-HHMMSS-{8char_uuid}
        self.actual_job_id = None  # Will be set after pipeline trigger
        
        # S3 bucket configuration
        self.s3_bucket_name = f"data-storage-msp-{environment}"

    def verify_existing_tenant(self):
        """Verify that the sitetechnology tenant exists and is configured"""
        print(f"üîß Verifying existing tenant: {self.test_tenant_id}")
        
        try:
            # Check if tenant exists in TenantServices table
            response = self.tenant_services_table.get_item(
                Key={
                    'tenant_id': self.test_tenant_id,
                    'service': 'connectwise'
                }
            )
            
            if 'Item' in response:
                tenant_config = response['Item']
                print(f"‚úÖ Tenant {self.test_tenant_id} found and configured")
                
                # Show available tables from endpoint configuration
                try:
                    import json
                    with open('mappings/integrations/connectwise_endpoints.json', 'r') as f:
                        endpoints_config = json.load(f)
                    
                    enabled_endpoints = []
                    for endpoint_path, config in endpoints_config.get('endpoints', {}).items():
                        if config.get('enabled', False):
                            table_name = config.get('table_name', endpoint_path.split('/')[-1])
                            enabled_endpoints.append(table_name)
                    
                    print(f"   Available tables from endpoint config: {enabled_endpoints}")
                except Exception as e:
                    print(f"   Warning: Could not load endpoint configuration: {str(e)}")
                
                return tenant_config
            else:
                print(f"‚ùå Tenant {self.test_tenant_id} not found in TenantServices table")
                return None
                
        except Exception as e:
            print(f"‚ùå Failed to verify tenant: {str(e)}")
            return None

    def trigger_pipeline(self, table_name: str = None, test_all_tables: bool = False):
        """Trigger the pipeline orchestrator"""
        if test_all_tables:
            print(f"üöÄ Triggering comprehensive pipeline test for all tables - tenant: {self.test_tenant_id}")
        else:
            actual_table = table_name or 'companies'
            print(f"üöÄ Triggering pipeline for tenant: {self.test_tenant_id}, table: {actual_table}")
        
        # Create test payload
        test_payload = {
            'tenant_id': self.test_tenant_id,
            'force_full_sync': False,
            'test_mode': True,
            'use_existing_tenant': True
        }
        
        # If testing all tables, don't specify table_name to trigger all enabled tables
        if not test_all_tables and table_name:
            test_payload['table_name'] = table_name
        elif not test_all_tables:
            test_payload['table_name'] = 'companies'  # Default fallback
        
        try:
            # Use synchronous invocation to get the job ID
            response = self.lambda_client.invoke(
                FunctionName=f'avesa-pipeline-orchestrator-{self.environment}',
                InvocationType='RequestResponse',  # Synchronous invocation to get job ID
                Payload=json.dumps(test_payload)
            )
            
            if response['StatusCode'] == 200:
                # Parse the response to get the job ID
                response_payload = json.loads(response['Payload'].read())
                self.actual_job_id = response_payload.get('job_id')
                
                if self.actual_job_id:
                    print(f"‚úÖ Pipeline triggered successfully")
                    print(f"   Job ID: {self.actual_job_id}")
                    if test_all_tables:
                        print(f"   Testing ALL enabled tables: companies, contacts, tickets, time_entries")
                    return True
                else:
                    print(f"‚ùå Pipeline triggered but no job ID returned")
                    print(f"   Response: {response_payload}")
                    return False
            else:
                print(f"‚ùå Pipeline trigger failed: HTTP {response['StatusCode']}")
                return False
                
        except Exception as e:
            print(f"‚ùå Failed to trigger pipeline: {str(e)}")
            return False

    def monitor_execution(self, timeout_minutes: int = 5):
        """Monitor pipeline execution progress"""
        if not self.actual_job_id:
            print("‚ùå No job ID available for monitoring")
            return 'ERROR', None
            
        print(f"üëÄ Monitoring execution for {timeout_minutes} minutes...")
        print(f"   Job ID: {self.actual_job_id}")
        
        start_time = time.time()
        timeout_seconds = timeout_minutes * 60
        
        while time.time() - start_time < timeout_seconds:
            try:
                # Check processing jobs table (composite key: job_id + tenant_id)
                response = self.processing_jobs_table.get_item(
                    Key={
                        'job_id': self.actual_job_id,
                        'tenant_id': self.test_tenant_id
                    }
                )
                
                if 'Item' in response:
                    job = response['Item']
                    status = job.get('status', 'UNKNOWN')
                    progress = job.get('progress', {})
                    
                    print(f"üìä Job Status: {status}")
                    if progress:
                        print(f"   Progress: {json.dumps(progress, indent=2)}")
                    
                    if status in ['COMPLETED', 'FAILED', 'completed', 'failed']:
                        return status, job
                        
                else:
                    print("‚è≥ Job not found yet, waiting...")
                
                time.sleep(10)  # Wait 10 seconds between checks
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Error monitoring execution: {str(e)}")
                time.sleep(10)
        
        print(f"‚è∞ Monitoring timeout after {timeout_minutes} minutes")
        return 'TIMEOUT', None

    def validate_results(self, job_data: Dict[str, Any]):
        """Validate pipeline execution results"""
        print("üîç Validating pipeline results...")
        
        if not job_data:
            print("‚ùå No job data to validate")
            return False
        
        # Check job completion
        status = job_data.get('status')
        if status in ['COMPLETED', 'completed']:
            print("‚úÖ Job completed successfully")
        elif status in ['FAILED', 'failed']:
            print("‚ùå Job failed")
            error_details = job_data.get('error_details', 'No error details available')
            print(f"   Error: {error_details}")
            return False
        else:
            print(f"‚ö†Ô∏è  Job in unexpected status: {status}")
            return False
        
        # Check progress details
        progress = job_data.get('progress', {})
        if progress:
            tables_processed = progress.get('tables_processed', 0)
            chunks_processed = progress.get('chunks_processed', 0)
            records_processed = progress.get('records_processed', 0)
            
            print(f"üìà Processing Summary:")
            print(f"   Tables: {tables_processed}")
            print(f"   Chunks: {chunks_processed}")
            print(f"   Records: {records_processed}")
            
            if chunks_processed > 0:
                print("‚úÖ Pipeline processed data successfully")
                return True
            else:
                print("‚ö†Ô∏è  No chunks were processed")
                return False
        else:
            print("‚ö†Ô∏è  No progress information available in job data")
            # If no progress in job data, check if chunks were processed
            print("üîç Checking chunk progress for validation...")
            return self.check_chunk_progress_for_validation()

    def check_chunk_progress(self):
        """Check chunk processing progress"""
        print("üîç Checking chunk progress...")
        
        try:
            # Query for chunks related to our test execution (job_id is the hash key)
            response = self.chunk_progress_table.query(
                KeyConditionExpression='job_id = :job_id',
                ExpressionAttributeValues={':job_id': self.actual_job_id}
            )
            
            chunks = response.get('Items', [])
            if chunks:
                print(f"üì¶ Found {len(chunks)} chunks:")
                for chunk in chunks:
                    chunk_id = chunk.get('chunk_id', 'Unknown')
                    status = chunk.get('status', 'Unknown')
                    records = chunk.get('records_processed', 0)
                    print(f"   Chunk {chunk_id}: {status} ({records} records)")
                return True
            else:
                print("üì¶ No chunks found for this execution")
                return False
                
        except Exception as e:
            print(f"‚ùå Error checking chunk progress: {str(e)}")
            return False

    def check_chunk_progress_for_validation(self):
        """Check chunk processing progress for validation purposes"""
        try:
            # Query for chunks related to our test execution (job_id is the hash key)
            response = self.chunk_progress_table.query(
                KeyConditionExpression='job_id = :job_id',
                ExpressionAttributeValues={':job_id': self.actual_job_id}
            )
            
            chunks = response.get('Items', [])
            if chunks:
                completed_chunks = [c for c in chunks if c.get('status') in ['completed', 'COMPLETED']]
                if completed_chunks:
                    print(f"‚úÖ Found {len(completed_chunks)} completed chunks - pipeline processed data successfully")
                    return True
                else:
                    print(f"‚ö†Ô∏è  Found {len(chunks)} chunks but none completed")
                    return False
            else:
                print("üì¶ No chunks found for this execution")
                return False
                
        except Exception as e:
            print(f"‚ùå Error checking chunk progress for validation: {str(e)}")
            return False

    def verify_s3_data_loading(self, comprehensive: bool = True) -> Dict[str, Any]:
        """Verify that data was actually loaded to S3 for all tables"""
        if not self.actual_job_id:
            print("‚ùå No job ID available for S3 verification")
            return {'success': False, 'tables': {}, 'total_files': 0}
            
        print("üîç Verifying S3 data loading for all tables...")
        
        # All ConnectWise tables that should be processed
        expected_tables = {
            'companies': 'company/companies',
            'contacts': 'company/contacts',
            'tickets': 'service/tickets',
            'time_entries': 'time/entries'  # Should use 'time_entries' as per connectwise_endpoints.json
        }
        
        verification_results = {
            'success': False,
            'tables': {},
            'total_files': 0,
            'total_size_bytes': 0
        }
        
        try:
            from datetime import datetime, timezone, timedelta
            # Look for files from the last 10 minutes to account for processing time
            cutoff_time = datetime.now(timezone.utc) - timedelta(minutes=10)
            
            for table_name, endpoint in expected_tables.items():
                print(f"\nüìä Checking table: {table_name}")
                table_result = {
                    'files_found': 0,
                    'total_size': 0,
                    'files': [],
                    'processed': False
                }
                
                # Check for files in the expected S3 path
                prefix = f"{self.test_tenant_id}/raw/connectwise/{table_name}/"
                
                try:
                    response = self.s3_client.list_objects_v2(
                        Bucket=self.s3_bucket_name,
                        Prefix=prefix,
                        MaxKeys=1000  # Increased to catch more files
                    )
                    
                    if 'Contents' in response:
                        recent_files = []
                        
                        for obj in response['Contents']:
                            # Check if this file was created recently
                            if obj['LastModified'] > cutoff_time:
                                file_info = {
                                    'key': obj['Key'],
                                    'size': obj['Size'],
                                    'last_modified': obj['LastModified']
                                }
                                recent_files.append(file_info)
                                table_result['total_size'] += obj['Size']
                        
                        table_result['files_found'] = len(recent_files)
                        table_result['files'] = recent_files
                        table_result['processed'] = len(recent_files) > 0
                        
                        if recent_files:
                            print(f"‚úÖ Found {len(recent_files)} recent files for '{table_name}':")
                            for file_info in recent_files[:5]:  # Show first 5 files
                                print(f"   üìÑ {file_info['key']} ({file_info['size']} bytes)")
                            if len(recent_files) > 5:
                                print(f"   ... and {len(recent_files) - 5} more files")
                        else:
                            print(f"‚ö†Ô∏è  No recent files found for '{table_name}' (within last 10 minutes)")
                    else:
                        print(f"‚ö†Ô∏è  No files found for '{table_name}' in S3")
                        
                except Exception as e:
                    print(f"‚ùå Error checking S3 for table '{table_name}': {str(e)}")
                    table_result['error'] = str(e)
                
                verification_results['tables'][table_name] = table_result
                verification_results['total_files'] += table_result['files_found']
                verification_results['total_size_bytes'] += table_result['total_size']
            
            # Determine overall success
            processed_tables = [name for name, result in verification_results['tables'].items() if result['processed']]
            verification_results['success'] = len(processed_tables) >= 3  # At least 3 out of 4 tables should have data
            verification_results['processed_tables'] = processed_tables
            
            print(f"\nüìà S3 Verification Summary:")
            print(f"   Tables processed: {len(processed_tables)}/4")
            print(f"   Total files found: {verification_results['total_files']}")
            print(f"   Total data size: {verification_results['total_size_bytes']} bytes")
            print(f"   Processed tables: {', '.join(processed_tables)}")
            
            if verification_results['success']:
                print("‚úÖ S3 verification successful - comprehensive data loading confirmed")
                # Verify content of sample files
                self._verify_s3_file_content_comprehensive(verification_results['tables'])
            else:
                print("‚ùå S3 verification failed - insufficient table coverage")
                
            return verification_results
                
        except Exception as e:
            print(f"‚ùå S3 verification error: {str(e)}")
            verification_results['error'] = str(e)
            return verification_results
    
    def _verify_s3_file_content(self, files: List[Dict[str, Any]]):
        """Verify the content and structure of S3 files"""
        print("üîç Verifying S3 file content...")
        
        for file_info in files:
            try:
                # Get file content
                response = self.s3_client.get_object(
                    Bucket=self.s3_bucket_name,
                    Key=file_info['key']
                )
                
                content = response['Body'].read()
                
                # Basic validation
                if file_info['key'].endswith('.json'):
                    # Validate JSON format
                    try:
                        data = json.loads(content)
                        if isinstance(data, list) and len(data) > 0:
                            print(f"‚úÖ {file_info['key']}: Valid JSON with {len(data)} records")
                        else:
                            print(f"‚ö†Ô∏è  {file_info['key']}: JSON file but no records")
                    except json.JSONDecodeError:
                        print(f"‚ùå {file_info['key']}: Invalid JSON format")
                elif file_info['key'].endswith('.parquet'):
                    print(f"‚úÖ {file_info['key']}: Parquet file ({file_info['size']} bytes)")
                else:
                    print(f"‚úÖ {file_info['key']}: File exists ({file_info['size']} bytes)")
                    
            except Exception as e:
                print(f"‚ùå Error verifying file {file_info['key']}: {str(e)}")

    def _verify_s3_file_content_comprehensive(self, tables_data: Dict[str, Any]):
        """Verify the content and structure of S3 files for all tables"""
        print("\nüîç Verifying S3 file content for all tables...")
        
        content_summary = {}
        
        for table_name, table_info in tables_data.items():
            if not table_info.get('processed', False):
                continue
                
            print(f"\nüìä Verifying content for table: {table_name}")
            table_content = {
                'files_verified': 0,
                'total_records': 0,
                'valid_files': 0,
                'errors': []
            }
            
            # Verify up to 3 files per table
            files_to_check = table_info.get('files', [])[:3]
            
            for file_info in files_to_check:
                try:
                    # Get file content
                    response = self.s3_client.get_object(
                        Bucket=self.s3_bucket_name,
                        Key=file_info['key']
                    )
                    
                    content = response['Body'].read()
                    table_content['files_verified'] += 1
                    
                    # Validate content based on file type
                    if file_info['key'].endswith('.json'):
                        try:
                            data = json.loads(content)
                            if isinstance(data, list):
                                record_count = len(data)
                                table_content['total_records'] += record_count
                                table_content['valid_files'] += 1
                                print(f"   ‚úÖ {file_info['key']}: {record_count} records")
                                
                                # Sample first record to verify structure
                                if record_count > 0:
                                    sample_record = data[0]
                                    if isinstance(sample_record, dict) and len(sample_record) > 0:
                                        print(f"      Sample fields: {list(sample_record.keys())[:5]}")
                            else:
                                print(f"   ‚ö†Ô∏è  {file_info['key']}: Not a JSON array")
                        except json.JSONDecodeError as e:
                            error_msg = f"Invalid JSON format: {str(e)}"
                            table_content['errors'].append(error_msg)
                            print(f"   ‚ùå {file_info['key']}: {error_msg}")
                    elif file_info['key'].endswith('.parquet'):
                        table_content['valid_files'] += 1
                        print(f"   ‚úÖ {file_info['key']}: Parquet file ({file_info['size']} bytes)")
                    else:
                        table_content['valid_files'] += 1
                        print(f"   ‚úÖ {file_info['key']}: File exists ({file_info['size']} bytes)")
                        
                except Exception as e:
                    error_msg = f"Error reading file: {str(e)}"
                    table_content['errors'].append(error_msg)
                    print(f"   ‚ùå {file_info['key']}: {error_msg}")
            
            content_summary[table_name] = table_content
        
        # Print overall content summary
        print(f"\nüìà Content Verification Summary:")
        total_records = sum(info.get('total_records', 0) for info in content_summary.values())
        total_valid_files = sum(info.get('valid_files', 0) for info in content_summary.values())
        total_errors = sum(len(info.get('errors', [])) for info in content_summary.values())
        
        print(f"   Total records across all tables: {total_records}")
        print(f"   Valid files verified: {total_valid_files}")
        print(f"   Errors encountered: {total_errors}")
        
        for table_name, info in content_summary.items():
            print(f"   {table_name}: {info.get('total_records', 0)} records, {info.get('valid_files', 0)} valid files")
        
        return content_summary

    def cleanup_test_data(self):
        """Clean up test execution data (but preserve tenant)"""
        print("üßπ Cleaning up test execution data...")
        
        try:
            # Note: We don't delete the tenant since it's a real tenant (sitetechnology)
            
            if self.actual_job_id:
                # Delete processing job (composite key: job_id + tenant_id)
                try:
                    self.processing_jobs_table.delete_item(
                        Key={
                            'job_id': self.actual_job_id,
                            'tenant_id': self.test_tenant_id
                        }
                    )
                    print("‚úÖ Processing job deleted")
                except:
                    pass  # Job might not exist
                
                # Delete chunk progress entries (composite key: job_id + chunk_id)
                try:
                    response = self.chunk_progress_table.query(
                        KeyConditionExpression='job_id = :job_id',
                        ExpressionAttributeValues={':job_id': self.actual_job_id}
                    )
                    
                    for chunk in response.get('Items', []):
                        self.chunk_progress_table.delete_item(
                            Key={
                                'job_id': chunk['job_id'],
                                'chunk_id': chunk['chunk_id']
                            }
                        )
                    print("‚úÖ Chunk progress entries deleted")
                except:
                    pass  # Chunks might not exist
            else:
                print("‚ö†Ô∏è  No job ID available for cleanup")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Cleanup error: {str(e)}")

    def check_recent_pipeline_executions(self):
        """Check recent pipeline executions to verify S3 data loading"""
        print("üîç Checking recent pipeline executions...")
        
        try:
            # Scan for recent processing jobs
            response = self.processing_jobs_table.scan(
                FilterExpression='attribute_exists(created_at)',
                Limit=10
            )
            
            recent_jobs = response.get('Items', [])
            if recent_jobs:
                print(f"üìä Found {len(recent_jobs)} recent jobs:")
                for job in recent_jobs:
                    job_id = job.get('job_id', 'Unknown')
                    tenant_id = job.get('tenant_id', 'Unknown')
                    status = job.get('status', 'Unknown')
                    created_at = job.get('created_at', 'Unknown')
                    print(f"   Job {job_id}: {status} (Tenant: {tenant_id}, Created: {created_at})")
                
                # Check S3 for the most recent completed job
                completed_jobs = [j for j in recent_jobs if j.get('status') in ['COMPLETED', 'completed']]
                if completed_jobs:
                    latest_job = max(completed_jobs, key=lambda x: x.get('created_at', ''))
                    self.actual_job_id = latest_job.get('job_id')
                    self.test_tenant_id = latest_job.get('tenant_id')
                    print(f"üîç Checking S3 data for latest completed job: {self.actual_job_id}")
                    return self.verify_s3_data_loading()
                else:
                    print("‚ö†Ô∏è  No completed jobs found")
                    return False
            else:
                print("‚ö†Ô∏è  No recent jobs found")
                return False
                
        except Exception as e:
            print(f"‚ùå Error checking recent executions: {str(e)}")
            return False

    def run_end_to_end_test(self, comprehensive: bool = False, table_name: str = None):
        """Run complete end-to-end test"""
        test_type = "Comprehensive Multi-Table" if comprehensive else "Single Table"
        print(f"üöÄ AVESA Phase 2 {test_type} End-to-End Pipeline Test")
        print("=" * 60)
        print(f"Region: {self.region}")
        print(f"Environment: {self.environment}")
        print(f"Test Time: {datetime.now().isoformat()}")
        print()
        
        success = False
        s3_results = None
        
        try:
            # Step 1: Verify existing tenant
            tenant_config = self.verify_existing_tenant()
            if not tenant_config:
                return False
            
            # Step 2: Trigger pipeline
            if not self.trigger_pipeline(table_name=table_name, test_all_tables=comprehensive):
                return False
            
            # Step 3: Monitor execution (longer timeout for comprehensive test)
            timeout = 10 if comprehensive else 5
            status, job_data = self.monitor_execution(timeout_minutes=timeout)
            
            # Step 4: Validate results
            if status in ['COMPLETED', 'FAILED', 'completed', 'failed']:
                success = self.validate_results(job_data)
            else:
                print(f"‚ùå Pipeline execution did not complete: {status}")
                success = False
            
            # Step 5: Check chunk progress
            self.check_chunk_progress()
            
            # Step 6: Verify S3 data loading
            if success:
                s3_results = self.verify_s3_data_loading(comprehensive=comprehensive)
                if not s3_results.get('success', False):
                    print("‚ö†Ô∏è  Pipeline completed but S3 verification failed")
                    success = False
            
        finally:
            # Always cleanup
            self.cleanup_test_data()
        
        print(f"\nüìä {test_type} End-to-End Test Summary")
        print("=" * 60)
        
        if success:
            print("üéâ End-to-end test PASSED!")
            print("‚úÖ Phase 2 pipeline is working correctly")
            if s3_results:
                print(f"‚úÖ S3 data loading verified for {len(s3_results.get('processed_tables', []))} tables")
                print(f"‚úÖ Total files generated: {s3_results.get('total_files', 0)}")
                print(f"‚úÖ Total data size: {s3_results.get('total_size_bytes', 0)} bytes")
        else:
            print("‚ùå End-to-end test FAILED!")
            print("‚ö†Ô∏è  Please review pipeline configuration and logs")
        
        return success

    def run_comprehensive_test(self):
        """Run comprehensive test for all ConnectWise tables"""
        print("üöÄ AVESA Comprehensive Multi-Table Pipeline Test")
        print("=" * 60)
        print(f"Region: {self.region}")
        print(f"Environment: {self.environment}")
        print(f"Test Time: {datetime.now().isoformat()}")
        print("Testing ALL ConnectWise tables: companies, contacts, tickets, time_entries")
        print()
        
        success = False
        s3_results = None
        test_summary = {
            'tenant_verified': False,
            'pipeline_triggered': False,
            'execution_completed': False,
            'chunks_processed': False,
            's3_verification': False,
            'tables_processed': [],
            'total_files': 0,
            'total_records': 0
        }
        
        try:
            # Step 1: Verify existing tenant and show available tables
            print("üìã Step 1: Verifying tenant configuration...")
            tenant_config = self.verify_existing_tenant()
            if not tenant_config:
                return False, test_summary
            test_summary['tenant_verified'] = True
            
            # Step 2: Trigger comprehensive pipeline (all tables)
            print("\nüöÄ Step 2: Triggering comprehensive pipeline...")
            if not self.trigger_pipeline(test_all_tables=True):
                return False, test_summary
            test_summary['pipeline_triggered'] = True
            
            # Step 3: Monitor execution with extended timeout
            print("\nüëÄ Step 3: Monitoring pipeline execution...")
            status, job_data = self.monitor_execution(timeout_minutes=15)
            
            # Step 4: Validate execution results
            print("\nüîç Step 4: Validating execution results...")
            if status in ['COMPLETED', 'FAILED', 'completed', 'failed']:
                execution_success = self.validate_results(job_data)
                test_summary['execution_completed'] = execution_success
                success = execution_success
            else:
                print(f"‚ùå Pipeline execution did not complete: {status}")
                success = False
            
            # Step 5: Check chunk progress for all tables
            print("\nüì¶ Step 5: Checking chunk processing...")
            chunks_success = self.check_chunk_progress()
            test_summary['chunks_processed'] = chunks_success
            
            # Step 6: Comprehensive S3 verification
            print("\nüóÑÔ∏è  Step 6: Comprehensive S3 data verification...")
            if success:
                s3_results = self.verify_s3_data_loading(comprehensive=True)
                s3_success = s3_results.get('success', False)
                test_summary['s3_verification'] = s3_success
                test_summary['tables_processed'] = s3_results.get('processed_tables', [])
                test_summary['total_files'] = s3_results.get('total_files', 0)
                
                if not s3_success:
                    print("‚ö†Ô∏è  Pipeline completed but comprehensive S3 verification failed")
                    success = False
            
        finally:
            # Always cleanup
            print("\nüßπ Cleaning up test data...")
            self.cleanup_test_data()
        
        # Generate comprehensive report
        self._generate_comprehensive_report(test_summary, s3_results)
        
        return success, test_summary

    def _generate_comprehensive_report(self, test_summary: Dict[str, Any], s3_results: Dict[str, Any]):
        """Generate a comprehensive test report"""
        print(f"\nüìä COMPREHENSIVE TEST REPORT")
        print("=" * 60)
        
        # Test execution summary
        print("üîç Test Execution Summary:")
        print(f"   ‚úÖ Tenant Verified: {test_summary['tenant_verified']}")
        print(f"   ‚úÖ Pipeline Triggered: {test_summary['pipeline_triggered']}")
        print(f"   ‚úÖ Execution Completed: {test_summary['execution_completed']}")
        print(f"   ‚úÖ Chunks Processed: {test_summary['chunks_processed']}")
        print(f"   ‚úÖ S3 Verification: {test_summary['s3_verification']}")
        
        # Table processing summary
        if s3_results:
            print(f"\nüìä Table Processing Summary:")
            processed_tables = test_summary.get('tables_processed', [])
            print(f"   Tables Successfully Processed: {len(processed_tables)}/4")
            print(f"   Processed Tables: {', '.join(processed_tables)}")
            
            # Detailed table results
            print(f"\nüìã Detailed Table Results:")
            for table_name, table_info in s3_results.get('tables', {}).items():
                status = "‚úÖ PROCESSED" if table_info.get('processed', False) else "‚ùå FAILED"
                files = table_info.get('files_found', 0)
                size = table_info.get('total_size', 0)
                print(f"   {table_name}: {status} ({files} files, {size} bytes)")
        
        # Overall assessment
        print(f"\nüéØ Overall Assessment:")
        if test_summary['s3_verification'] and len(test_summary.get('tables_processed', [])) >= 3:
            print("   üéâ COMPREHENSIVE TEST PASSED!")
            print("   ‚úÖ Multi-table ingestion pipeline is working correctly")
            print("   ‚úÖ All major ConnectWise data types are being processed")
            print("   ‚úÖ S3 data loading verified across multiple tables")
        else:
            print("   ‚ùå COMPREHENSIVE TEST FAILED!")
            print("   ‚ö†Ô∏è  Multi-table processing needs attention")
            if len(test_summary.get('tables_processed', [])) < 3:
                print(f"   ‚ö†Ô∏è  Only {len(test_summary.get('tables_processed', []))} tables processed (minimum 3 expected)")

def main():
    """Main execution"""
    import argparse
    
    parser = argparse.ArgumentParser(description='AVESA End-to-End Pipeline Tester')
    parser.add_argument('--region', default='us-east-2', help='AWS region')
    parser.add_argument('--environment', default='dev', help='Environment (dev/staging/prod)')
    parser.add_argument('--check-recent', action='store_true',
                       help='Check recent pipeline executions and verify S3 data')
    parser.add_argument('--table', help='Specific table to test (companies, contacts, tickets, time_entries)')
    parser.add_argument('--comprehensive', action='store_true',
                       help='Run comprehensive test for ALL ConnectWise tables')
    
    args = parser.parse_args()
    
    tester = EndToEndPipelineTester(region=args.region, environment=args.environment)
    
    if args.check_recent:
        print("üîç Checking recent pipeline executions and S3 data...")
        success = tester.check_recent_pipeline_executions()
    elif args.comprehensive:
        print("üöÄ Running comprehensive multi-table test...")
        success, test_summary = tester.run_comprehensive_test()
    else:
        # Run single table test or comprehensive based on table parameter
        if args.table:
            success = tester.run_end_to_end_test(comprehensive=False, table_name=args.table)
        else:
            # Default to comprehensive test if no specific table specified
            success = tester.run_end_to_end_test(comprehensive=True)
    
    exit(0 if success else 1)

if __name__ == "__main__":
    main()