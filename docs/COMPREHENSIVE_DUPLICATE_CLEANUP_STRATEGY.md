# Comprehensive Duplicate Cleanup Strategy

## üö® Critical Data Quality Issues Identified

**Date**: June 21, 2025  
**Severity**: CRITICAL  
**Impact**: Multiple types of duplicates affecting data integrity

## üìä Duplicate Issues Summary

### 1. **Exact Duplicates (Primary Issue)**
- **Records**: 44,000 exact duplicates (97.78% of data)
- **Pattern**: Same tenant_id, ID, last_updated, AND is_current
- **Cause**: 44 concurrent Lambda functions processing same data
- **Impact**: Massive storage waste, query performance degradation

### 2. **SCD Type 2 Violations (Secondary Issue)**  
- **Records**: 999 SCD violations
- **Pattern**: Same tenant_id, ID, last_updated BUT different is_current values
- **Cause**: Race condition in SCD Type 2 logic during concurrent processing
- **Impact**: Violates SCD Type 2 integrity, incorrect current record identification

## üõ†Ô∏è Cleanup Strategy

### **Phase 1: Exact Duplicate Cleanup (Highest Priority)**

**Tool**: [`scripts/cleanup-exact-duplicates.py`](scripts/cleanup-exact-duplicates.py)

**What it does**:
- Removes 43,000 exact duplicate records
- Preserves 1,000 unique records (oldest by effective_date)
- Generates 860 cleanup queries in safe batches

**Command**:
```bash
python3 scripts/cleanup-exact-duplicates.py --execute --save-report
```

**Safety measures**:
- Dry run by default
- Requires exact confirmation: "DELETE DUPLICATES"
- Preserves oldest record in each duplicate group
- Batch processing to avoid query size limits

### **Phase 2: SCD Violation Cleanup (Secondary Priority)**

**Tool**: [`scripts/cleanup-scd-violations.py`](scripts/cleanup-scd-violations.py)

**What it does**:
- Removes 999 records with `is_current=false` that violate SCD logic
- Keeps records with `is_current=true` for same ID+last_updated
- Generates 20 cleanup queries in safe batches

**Command**:
```bash
python3 scripts/cleanup-scd-violations.py --execute --save-report
```

**Safety measures**:
- Dry run by default
- Requires exact confirmation: "FIX SCD VIOLATIONS"
- Preserves current records, removes non-current duplicates
- Maintains SCD Type 2 integrity

## üìã Execution Plan

### **Step 1: Pre-Cleanup Verification**
```bash
# Verify current state
python3 scripts/investigate-exact-duplicates.py --save-report
python3 scripts/investigate-scd-violations.py --save-report
```

### **Step 2: Execute Exact Duplicate Cleanup**
```bash
# This removes the bulk of the duplicates (43,000 records)
python3 scripts/cleanup-exact-duplicates.py --execute --save-report
```

**Expected result**: Reduces from 45,000 to ~2,000 records

### **Step 3: Execute SCD Violation Cleanup**
```bash
# This fixes the remaining SCD logic violations (999 records)
python3 scripts/cleanup-scd-violations.py --execute --save-report
```

**Expected result**: Reduces from ~2,000 to ~1,000 records

### **Step 4: Post-Cleanup Verification**
```bash
# Verify cleanup was successful
python3 scripts/investigate-exact-duplicates.py
python3 scripts/investigate-scd-violations.py
```

**Expected result**: 0 duplicates, 0 SCD violations

## üîç Data Impact Analysis

### **Before Cleanup**
- **Total Records**: 45,000
- **Unique Records**: ~1,000 (2.22%)
- **Exact Duplicates**: 44,000 (97.78%)
- **SCD Violations**: 999 groups
- **Storage Efficiency**: 2.22%
- **Query Performance**: Degraded by 44x

### **After Cleanup**
- **Total Records**: ~1,000
- **Unique Records**: ~1,000 (100%)
- **Exact Duplicates**: 0 (0%)
- **SCD Violations**: 0 groups
- **Storage Efficiency**: 100%
- **Query Performance**: Optimal

### **Storage Savings**
- **Records Removed**: 44,000 (97.78% reduction)
- **Storage Savings**: ~97.78% reduction in table size
- **Query Performance**: 44x improvement
- **Cost Savings**: Significant reduction in ClickHouse storage costs

## üöÄ Pipeline Fixes Implemented

### **1. Idempotency Logic**
**File**: [`src/clickhouse/data_loader/lambda_function.py`](src/clickhouse/data_loader/lambda_function.py)

**Fix**: Added `check_if_already_processed()` function
- Compares S3 file timestamps with last processed time
- Skips processing if all files already processed
- Prevents multiple Lambda instances from processing same data

### **2. Schema Alignment**
**Fix**: Corrected column name from `created_date` to `date_entered`
- Eliminates INSERT failures that trigger retries
- Reduces Lambda execution errors
- Prevents retry-induced duplication

### **3. Improved Staging Logic**
**Fix**: Enhanced staging table names with microsecond precision
- Prevents staging table name collisions
- Maintains atomic SCD Type 2 operations
- Each Lambda gets unique staging table

## üìà Success Metrics

### **Data Quality KPIs**
- **Exact Duplicates**: 0% (target: 0%, current: 97.78%)
- **SCD Violations**: 0 groups (target: 0, current: 999)
- **Data Freshness**: < 1 hour lag
- **Pipeline Success Rate**: > 99.5%

### **Performance KPIs**
- **Storage Utilization**: 100% (target: >95%, current: 2.22%)
- **Query Performance**: Baseline (target: <2x degradation, current: 44x)
- **Lambda Execution Time**: < 5 minutes
- **Duplicate Detection**: Real-time alerts

### **Operational KPIs**
- **Idempotency Hit Rate**: Track skipped processing
- **Lambda Concurrent Executions**: Monitor for spikes
- **Data Loading Errors**: < 0.1%
- **Manual Interventions**: < 1 per month

## üîß Monitoring and Alerting

### **Real-Time Monitoring**
- **Duplicate Detection**: [`scripts/monitor-clickhouse-duplicates.py`](scripts/monitor-clickhouse-duplicates.py)
- **SCD Violation Detection**: Custom alerts for same ID+last_updated with different is_current
- **Lambda Concurrency**: CloudWatch metrics for concurrent executions
- **Data Quality Pipeline**: [`infrastructure/monitoring/data_quality_pipeline_monitoring.py`](infrastructure/monitoring/data_quality_pipeline_monitoring.py)

### **Alert Thresholds**
- **Exact Duplicates**: Alert if > 0 found
- **SCD Violations**: Alert if > 0 found
- **Lambda Concurrency**: Alert if > 5 concurrent executions for same tenant
- **Processing Failures**: Alert if > 1% failure rate

## üö® Risk Mitigation

### **Backup Strategy**
- **Pre-Cleanup Snapshot**: Take ClickHouse backup before cleanup
- **Incremental Backups**: Daily backups during stabilization period
- **Rollback Plan**: Documented procedure to restore from backup

### **Validation Strategy**
- **Row Count Validation**: Verify expected record counts after cleanup
- **Data Integrity Checks**: Validate SCD Type 2 logic is working
- **Business Logic Validation**: Ensure analytics queries return expected results
- **Performance Testing**: Verify query performance improvements

### **Rollback Procedures**
1. **Stop all data loading pipelines**
2. **Restore from pre-cleanup backup**
3. **Verify data integrity**
4. **Resume pipelines with fixes**

## üìû Escalation Matrix

| Issue Type | Severity | Response Time | Escalation Path |
|------------|----------|---------------|-----------------|
| Cleanup Failure | Critical | Immediate | Data Engineering Team ‚Üí Engineering Manager |
| Data Loss | Critical | Immediate | Data Engineering Team ‚Üí CTO |
| Performance Degradation | High | 1 hour | Data Engineering Team ‚Üí DevOps |
| SCD Violations | Medium | 4 hours | Data Engineering Team |
| Monitoring Alerts | Low | 24 hours | Data Engineering Team |

## üìÖ Timeline

### **Immediate (Today)**
- [x] Investigate and identify duplicate patterns
- [x] Create cleanup tools and strategies
- [x] Test cleanup tools in dry-run mode
- [ ] Execute exact duplicate cleanup
- [ ] Execute SCD violation cleanup
- [ ] Verify cleanup success

### **Short Term (This Week)**
- [ ] Deploy pipeline fixes
- [ ] Implement comprehensive monitoring
- [ ] Document operational procedures
- [ ] Conduct performance testing

### **Long Term (Next Sprint)**
- [ ] Add automated duplicate prevention
- [ ] Implement circuit breaker patterns
- [ ] Optimize Lambda concurrency settings
- [ ] Prepare for multi-tenant scaling

---

**Document Owner**: Data Engineering Team  
**Last Updated**: June 21, 2025  
**Review Cycle**: Daily during cleanup phase, weekly during stabilization